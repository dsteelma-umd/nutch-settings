# Nutch

## Installation

1) Download the Apache Nutch source (apache-nutch-1.1X-src.zip).

2) Extract it and change directory:

```
> tar xf apache-nutch-1.1X-src.zip
> cd apache-nutch-1.1X/
```

3) Run ant:

```
> ant
```

4) [OPTIONAL] Setup permissions and JAVA_HOME:

```
> chmod +x bin/nutch
> export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:bin/java::")
```

5) Our Nutch build is now ready at:

```
> cd runtime/local
```

I will refer to this henceforth directory as ```${NUTCH_DIR}```

## Configuration

We need to customize three properties at minimum to crawl our first website.

### Crawl Properties

The default nutch crwaler properties are found in ```${NUTCH_DIR}/conf/nutch-default.xml``` and are overriden by the properties in ```${NUTCH_DIR}/conf/nutch-site.xml```. 

We will need to add one property to ```conf/nutch-site.xml``` to initiate the search.

```
<property>
 <name>http.agent.name</name>
 <value>Quick Search Crawler</value>
</property>
```

### Initial seed list

A URL seed list includes a list of websites, one-per-line, which nutch will look to crawl. We can create this list by using the following commands and adding the seed urls:

```
> mkdir -p ${NUTCH_DIR}/urls
> cd ${NUTCH_DIR}/urls
> vi seed.txt
```

Our initial seed file will only contain the UMD Libraries Homepage:

```
https://www.lib.umd.edu/
```

### Regular Expression Filters

Nutch will go through the content in ```${NUTCH_DIR}/conf/regex-urlfilter.txt``` line by line and stop at the first matching rule. If the regular expression in a line matches the URL, it will include (```+```) or exclude (```-```) the URL depending on the sign, and skip the remaining lines. If there is no matching regular expression by the end of the file, it will discard the URL.

Our filter will only contain the https versions of the UMD Libraries URLs and discard ```dbfinder``` and other URLs:

```
# reject DBfinder URLs
-^https?://(www\.)*lib\.umd\.edu/dbfinder

# accept https UMD Libraries URLs
+^https://(www\.)?lib\.umd\.edu

# reject anything else
-.
```

Your searcher configurations are ready now.


## Crawling

### Simple

In order to crawl the website, you will need to run the following command from 
```${NUTCH_DIR}```:

```
> bin/crawl -s urls/ LibCrawl/  10
```

This will generate the seed list from files in the ```urls``` directory, generate the crawl and link databases in the ```LibCrawl``` directory, and run this search upto depth ```10```.

### Indexed

You can also run the same crawl and index it to the Solr URL:

```
> bin/crawl -i -D solr.server.url=http://localhost:8983/solr/nutch -s urls/ LibCrawl/  10
```

This will also index the results on the Solr URL ```http://localhost:8983/solr/nutch```.


## Analysis

The following commands can be used to analyze databases generated by crawls:

### Stats

This command will print out the crawl statistics to stdout:

```
> bin/nutch readdb <folder>/crawldb/ -stats
```

The head and tail end of this output is most useful for a quick snippet of the search:

```
Statistics for CrawlDb: <folder>/crawldb/
TOTAL urls:	636
shortest fetch interval:	00:10:00
avg fetch interval:	00:10:06
longest fetch interval:	00:15:00
...
status 1 (db_unfetched):	310
status 2 (db_fetched):	297
status 3 (db_gone):	14
status 4 (db_redir_temp):	3
status 5 (db_redir_perm):	9
status 6 (db_notmodified):	1
status 7 (db_duplicate):	2
CrawlDb statistics: done
```

### URL Dumps

This command will generate a detailed dump of the URLs traversed:

```
> bin/nutch readdb <folder>/crawldb/ -dump <folder>_urls
```

The dump file contains detailed information on each link including URL, hash, timestamps, and HTTP status codes. 

There are six main types of Nutch statuses associated with each crawl:

1) Unfetched: ```http://lib.umd.edu/directory``` will be fetched on the next cycle (depth)

```
http://lib.umd.edu/directory	Version: 7
Status: 1 (db_unfetched)
...
Metadata: 
```

2) Fetched: ```https://www.lib.umd.edu/``` was successfully fetched

```
https://www.lib.umd.edu/	Version: 7
Status: 2 (db_fetched)
...
Metadata: 
 	_pst_=success(1), lastModified=0
	_rs_=73
	Content-Type=text/html
	nutch.protocol.code=200
```

3) Permanent Redirects: ```https://www.lib.umd.edu/help.html``` was temporarily redirected to ```http://umd.libanswers.com/```

```
https://www.lib.umd.edu/help.html Version: 7
Status: 5 (db_redir_perm)
...
Metadata: 
 	_pst_=moved(12), lastModified=0: http://umd.libanswers.com/
	_rs_=58
	Content-Type=text/html
	nutch.protocol.code=301
```

4) Temporary Redirects: ```http://lib.umd.edu/``` was temporarily redirected to ```https://www.lib.umd.edu/```

```
http://lib.umd.edu/	Version: 7
Status: 4 (db_redir_temp)
...
Metadata: 
 	_pst_=temp_moved(13), lastModified=0: https://www.lib.umd.edu/
	_rs_=3
	Content-Type=text/html
	nutch.protocol.code=302
``` 

5) Not Found:

```
https://www.lib.umd.edu/pagenotfound	Version: 7
Status: 3 (db_gone)
...
Metadata: 
 	_pst_=notfound(14), lastModified=0: https://www.lib.umd.edu/pagenotfound
	_rs_=581
	nutch.protocol.code=404
```

6) Denied by ```robots.txt```:

```
https://www.lib.umd.edu/directory/facet/ Version: 7
Status: 3 (db_gone)
...
Metadata: 
 	_pst_=robots_denied(18), lastModified=0
```

### Inlink Dumps

This command will generate a detailed dump of the inlinks to each URL traversed:

```
> bin/nutch readlinkdb <folder>/linkdb/ -dump <folder>_links
```

The output will have the following format:

```
https://www.lib.umd.edu/	Inlinks:
 fromUrl: https://www.lib.umd.edu/find anchor: University of Maryland Libraries
 fromUrl: https://www.lib.umd.edu/special/guides/rare-books-art anchor: Home
 fromUrl: https://www.lib.umd.edu/groupvisits anchor: Home
 ...
```

This shows all the links pointing to ```https://www.lib.umd.edu/```