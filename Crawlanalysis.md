# Crawlanalysis

## Crawling

### Simple

In order to crawl the website, you will need to run the following command from 
```${NUTCH_DIR}```:

```
> bin/crawl -s urls/ LibCrawl/  10
```

This will generate the seed list from files in the ```urls``` directory, generate the crawl and link databases in the ```LibCrawl``` directory, and run this search upto depth ```10```.

### Indexed

You can also run the same crawl and index it to the Solr URL:

```
> bin/crawl -i -D solr.server.url=http://localhost:8983/solr/nutch -s urls/ LibCrawl/  10
```

This will also index the results on the Solr URL ```http://localhost:8983/solr/nutch```.

## Querying Solr

The crawl results can be viewed on the browser by going to:

```
http://localhost:8983/solr/#/nutch/query
```

The JSOn API of the same query can be found at:

```
http://localhost:8983/solr/nutch/select?q=*:*&wt=json
```

Here are a few important parameters in the Solr Console:

Parameter | Default | Description | Example
--- | ------ | --- | ---
q | \*:\* | The Query String | url:news
sort | score desc | Sorts the response to a query in either ascending or descending order based on the responseâ€™s score or another specified characteristic. | score desc, url asc
start | 0 | Specifies an offset into the responses at which Solr should begin displaying content. | 4
rows | 10 | Controls how many rows of responses are displayed at a time | 3

More information regarding parameters can be found [here][1]. 


## Analysis

The following commands can be used to analyze databases generated by crawls:

### Stats

This command will print out the crawl statistics to stdout:

```
> bin/nutch readdb LibCrawl/crawldb/ -stats
```

The head and tail end of this output gives a quick snippet of the search:

```
Statistics for CrawlDb: LibCrawl/crawldb/
TOTAL urls:	636
shortest fetch interval:	00:10:00
avg fetch interval:	00:10:06
longest fetch interval:	00:15:00
...
status 1 (db_unfetched):	310
status 2 (db_fetched):	297
status 3 (db_gone):	14
status 4 (db_redir_temp):	3
status 5 (db_redir_perm):	9
status 6 (db_notmodified):	1
status 7 (db_duplicate):	2
CrawlDb statistics: done
```

### URL Dumps

This command will generate a detailed dump of the URLs traversed:

```
> bin/nutch readdb LibCrawl/crawldb/ -dump LibCrawl_urls
```

The dump file contains detailed information on each link including URL, hash, timestamps, and HTTP status codes. 

There are six main types of Nutch statuses associated with each crawl:

1) Unfetched: ```http://lib.umd.edu/directory``` will be fetched on the next fetch cycle (depth)

```
http://lib.umd.edu/directory	Version: 7
Status: 1 (db_unfetched)
...
Metadata: 
```

2) Fetched: ```https://www.lib.umd.edu/``` was successfully fetched

```
https://www.lib.umd.edu/	Version: 7
Status: 2 (db_fetched)
...
Metadata: 
 	_pst_=success(1), lastModified=0
	_rs_=73
	Content-Type=text/html
	nutch.protocol.code=200
```

3) Permanent Redirects: ```https://www.lib.umd.edu/help.html``` was permanently redirected to ```http://umd.libanswers.com/```

```
https://www.lib.umd.edu/help.html Version: 7
Status: 5 (db_redir_perm)
...
Metadata: 
 	_pst_=moved(12), lastModified=0: http://umd.libanswers.com/
	_rs_=58
	Content-Type=text/html
	nutch.protocol.code=301
```

4) Temporary Redirects: ```http://lib.umd.edu/``` was temporarily redirected to ```https://www.lib.umd.edu/```

```
http://lib.umd.edu/	Version: 7
Status: 4 (db_redir_temp)
...
Metadata: 
 	_pst_=temp_moved(13), lastModified=0: https://www.lib.umd.edu/
	_rs_=3
	Content-Type=text/html
	nutch.protocol.code=302
``` 

5) Not Found: Mostly HTTP 404s. See the ```nutch.protocol.code``` for more details.

```
https://www.lib.umd.edu/pagenotfound	Version: 7
Status: 3 (db_gone)
...
Metadata: 
 	_pst_=notfound(14), lastModified=0: https://www.lib.umd.edu/pagenotfound
	_rs_=581
	nutch.protocol.code=404
```

6) Denied by respecting ```robots.txt```:

```
https://www.lib.umd.edu/directory/facet/ Version: 7
Status: 3 (db_gone)
...
Metadata: 
 	_pst_=robots_denied(18), lastModified=0
```

### Inlink Dumps

This command will generate a detailed dump of the inlinks to each URL traversed:

```
> bin/nutch readlinkdb LibCrawl/linkdb/ -dump LibCrawl_links
```

The output will have the following format:

```
https://www.lib.umd.edu/	Inlinks:
 fromUrl: https://www.lib.umd.edu/find anchor: University of Maryland Libraries
 fromUrl: https://www.lib.umd.edu/special/guides/rare-books-art anchor: Home
 fromUrl: https://www.lib.umd.edu/groupvisits anchor: Home
 ...
```

This shows all the links with anchor texts pointing to ```https://www.lib.umd.edu/```


[1]: https://lucene.apache.org/solr/guide/6_6/common-query-parameters.html